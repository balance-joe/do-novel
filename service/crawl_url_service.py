import random
import re
import logging
from urllib.parse import urlparse, urljoin, unquote,quote, urlunparse
import httpx
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Tuple
import asyncio
import aiohttp
import posixpath
import hashlib, os
from lxml import html
from lxml.etree import Comment
import tldextract
from service.fetch_utils import RequestManager

logger = logging.getLogger('CrawlUrlService')

class CrawlUrlService:
    CHINESE_PATTERN = re.compile(r'[\u4e00-\u9fa5\u3400-\u4dbf\U00020000-\U0002a6df\U0002a700-\U0002b73f\U0002b740-\U0002b81f\U0002b820-\U0002ceaf]+')

    def __init__(self, proxies=None):

        self.session = None

        # ç›´æ¥æŒ‡å®šå¿«ä»£ç†
        self.proxies = [
            # "http://q808.kdltps.com:15818",   # åªéœ€æ›¿æ¢æˆä½ ä»¬çš„éš§é“åŸŸåå’Œç«¯å£
        ]
        
        self.req_mgr = RequestManager(proxies=self.proxies)

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self  # è¿”å›è‡ªèº«ä¾› async with ä½¿ç”¨

    async def __aexit__(self, exc_type, exc, tb):
        await self.session.close()
        
    def get_url_data(self, url: str, main_domain: str = "henu.edu.cn") -> List[Dict]:
        """
        è·å–æŒ‡å®šURLé¡µé¢ä¸­çš„æ‰€æœ‰æœ‰æ•ˆé“¾æ¥
        
        å‚æ•°:
            url: è¦æŠ“å–çš„URL
            main_domain: ä¸»åŸŸåï¼Œç”¨äºé“¾æ¥è¿‡æ»¤
        
        è¿”å›:
            åŒ…å«æœ‰æ•ˆé“¾æ¥(titleå’Œlink)çš„å­—å…¸åˆ—è¡¨
        """
        try:
            # è§£æURLè·å–åŸºç¡€ä¿¡æ¯
            url_info = urlparse(url)
            fetch_domain = f"{url_info.scheme}://{url_info.netloc}"
            
            # è·å–é¡µé¢æ‰€æœ‰é“¾æ¥
            links = self.fetch_page_links(url)
            logger.info(f"æŠ“å–æˆåŠŸ: {url}")
        except Exception as e:
            logger.error(f"æŠ“å–å¤±è´¥: {url} - {str(e)}")
            return []
        
        # å¤„ç†å¹¶è¿‡æ»¤é“¾æ¥
        valid_links = []
        for item in links:
            # å¤„ç†é“¾æ¥é¡¹ï¼ˆä¸­æ–‡ç¼–ç ã€ç›¸å¯¹è·¯å¾„è½¬æ¢ç­‰ï¼‰
            processed = self.process_link_item(item, url)
            if not processed:
                continue
                
            # éªŒè¯é“¾æ¥æœ‰æ•ˆæ€§
            if self.is_valid_link(processed) and self.is_valid_http_link(processed["link"], main_domain):
                valid_links.append(processed)
        
        # å»é‡åè¿”å›
        return self.deduplicate_by_link(valid_links)

    def fetch_page_links(self, url: str) -> List[Dict]:
        """
        è·å–æŒ‡å®šURLé¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥
        """
        try:
            html = self.req_mgr.request_sync(url)  # <-- ä½¿ç”¨ fetch_utils.py ç®¡ç†è¯·æ±‚
            if not html:
                logger.error(f"è·å–é¡µé¢å¤±è´¥: {url}")
                return []

            # è§£æHTMLè·å–é“¾æ¥
            return self.parse_html_links(html, url)
        except Exception as e:
            logger.error(f"è·å–é¡µé¢é“¾æ¥å¤±è´¥: {url} - {str(e)}")
            return []
        finally:
            # å®‰å…¨å…³é—­ä¼šè¯
            try:
                if self.req_mgr and self.req_mgr.session and not self.req_mgr.session.closed:
                    import asyncio
                    asyncio.run(self.req_mgr.session.close())
            except Exception as close_err:
                logger.warning(f"å…³é—­ aiohttp session æ—¶å‡ºé”™: {close_err}")

    
    async def async_fetch_multiple(self, urls: List[str], retry: int = 3) -> List[Dict]:
        """
        å¼‚æ­¥æ‰¹é‡è·å–å¤šä¸ª URL é¡µé¢å†…å®¹
        - è‡ªåŠ¨å…³é—­ session
        - å†…éƒ¨æ”¯æŒ RequestManager çš„å¤ç”¨ã€é™é€Ÿä¸é‡è¯•
        """
        if not self.req_mgr:
            raise RuntimeError("RequestManager æœªåˆå§‹åŒ–")

        results = []
        try:
            async def fetch_one(url):
                html = await self.req_mgr.request_async(url, retry=retry)
                return {"url": url, "html": html}

            tasks = [fetch_one(url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=False)
            return results

        except Exception as e:
            print(f"[AsyncBatch] æ‰¹é‡æŠ“å–å¼‚å¸¸: {e}")
            return []

        finally:
            # ğŸ”’ ä¿è¯ session åœ¨ä»»åŠ¡ç»“æŸåå…³é—­
            try:
                await self.req_mgr.close()
            except Exception as close_err:
                print(f"[AsyncBatch] å…³é—­ aiohttp session å‡ºé”™: {close_err}")

    async def async_fetch_single(self, url: str, retry: int = 3) -> Dict:
        """
        å¼‚æ­¥æŠ“å–å•ä¸ªURLå†…å®¹ï¼Œæ”¯æŒé‡è¯•
        """
        result_list = await self.async_fetch_multiple([url], retry=retry)
        return result_list[0] if result_list else {'url': url, 'html': None}

    async def async_fetch_in_batches(self, urls: List[str], batch_size: int = 10, retry: int = 3, auto_close: bool = True):
        """
        åˆ†æ‰¹å¼‚æ­¥æŠ“å–ï¼ˆé˜²å° + é™æµå‹å¥½ç‰ˆï¼‰
        - æ”¯æŒè‡ªåŠ¨å…³é—­ session
        - æ¯æ‰¹ä¹‹é—´éšæœºå»¶æ—¶ï¼Œé˜²æ­¢è¢«å°
        - æ‰¹æ¬¡å¼‚å¸¸ä¸å½±å“æ•´ä½“æ‰§è¡Œ
        """
        if not self.req_mgr:
            raise RuntimeError("RequestManager æœªåˆå§‹åŒ–")

        all_results = []

        try:
            for i in range(0, len(urls), batch_size):
                batch = urls[i:i + batch_size]

                async def fetch_one(url):
                    try:
                        html = await self.req_mgr.request_async(url, retry=retry)
                        return {"url": url, "html": html}
                    except Exception as e:
                        print(f"[AsyncBatch] æŠ“å–å¤±è´¥: {url} - {e}")
                        return {"url": url, "html": None}

                # âš™ï¸ æ‰¹æ¬¡å¹¶å‘æ‰§è¡Œ
                batch_results = await asyncio.gather(*[fetch_one(u) for u in batch], return_exceptions=False)
                all_results.extend(batch_results)

                # ğŸ•’ æ‰¹æ¬¡é—´å»¶æ—¶ï¼ˆ2~5ç§’ï¼‰
                delay = random.uniform(2, 5)
                print(f"[AsyncBatch] æ‰¹æ¬¡ {i // batch_size + 1} å®Œæˆï¼Œä¼‘çœ  {delay:.2f}s...")
                await asyncio.sleep(delay)

            return all_results

        finally:
            # âœ… ä»»åŠ¡å®Œæˆåå®‰å…¨å…³é—­ session
            if auto_close:
                try:
                    await self.req_mgr.close()
                except Exception as e:
                    print(f"[AsyncBatch] å…³é—­ session å‡ºé”™: {e}")

    async def fetch_and_parse_multiple(self, urls: List[str], main_domain: str) -> List[Dict]:
        """
        æ‰¹é‡å¼‚æ­¥æŠ“å–å¹¶è§£æå¤šä¸ªURLçš„é“¾æ¥
        
        å‚æ•°:
            urls: URLåˆ—è¡¨
            main_domain: ä¸»åŸŸåç”¨äºé“¾æ¥è¿‡æ»¤
            
        è¿”å›:
            åŒ…å«æ‰€æœ‰æœ‰æ•ˆé“¾æ¥çš„å­—å…¸åˆ—è¡¨
        """
        # 1. å¼‚æ­¥è·å–æ‰€æœ‰URLçš„HTMLå†…å®¹
        results = await self.async_fetch_multiple(urls)
        
        # 2. è§£ææ¯ä¸ªé¡µé¢çš„é“¾æ¥
        all_links = []
        for result in results:
            if not result['html']:
                continue
                
            # è§£æHTMLä¸­çš„é“¾æ¥
            links = self.parse_html_links(result['html'], result['url'])
            
            # å¤„ç†æ¯ä¸ªé“¾æ¥é¡¹
            for link_data in links:
                processed = self.process_link_item(link_data, result['url'])
                if not processed:
                    continue
                    
                # éªŒè¯é“¾æ¥æœ‰æ•ˆæ€§
                if self.is_valid_http_link(processed['link'], main_domain):
                    all_links.append(processed)
        
        # 3. å»é‡åè¿”å›
        return self.deduplicate_by_link(all_links)


    def parse_html_links(self, html: str, base_url: str) -> List[Dict]:
        """
        è§£æHTMLå†…å®¹ä¸­çš„é“¾æ¥
        
        å‚æ•°:
            html: HTMLå†…å®¹
            base_url: åŸºç¡€URLï¼Œç”¨äºç›¸å¯¹è·¯å¾„è½¬æ¢
        
        è¿”å›:
            åŒ…å«é“¾æ¥(titleå’Œlink)çš„å­—å…¸åˆ—è¡¨
        """
        if not html:
            return []
        
        soup = BeautifulSoup(html, "lxml")
        links = []
        
        for a_tag in soup.find_all("a", href=True):
            href = a_tag.get("href", "").strip()
            
            # å…ˆå– title å±æ€§ï¼Œæ²¡æœ‰å°±å–æ–‡æœ¬
            title = a_tag.get("title", "").strip()
            if not title:
                title = a_tag.get_text().strip()
            
            if not title or not href:
                continue
                
            links.append({
                "link": href,
                "title": title
            })
            
        return links

    def process_link_item(self, item: Dict, base_url: str) -> Optional[Dict]:
        """
        å¤„ç†é“¾æ¥é¡¹
        
        å‚æ•°:
            item: é“¾æ¥é¡¹å­—å…¸
            base_url: åŸºç¡€URL
            
        è¿”å›:
            å¤„ç†åçš„é“¾æ¥å­—å…¸æˆ–None
        """
        if not item.get('link'):
            return None
            
        # ä¸­æ–‡URLç¼–ç å¤„ç†
        link = self.CHINESE_PATTERN.sub(
            lambda m: ''.join(f'%{ord(c):02X}' for c in m.group()),
            item['link']
        )
        
        # ç›¸å¯¹è·¯å¾„è½¬ç»å¯¹è·¯å¾„
        if not link.startswith(('http://', 'https://')):
            link = self.resolve_url(base_url, link)
        
        return {
            'link': link,
            'title': item.get('title', '').strip()
        }

    def encode_chinese_url(self, url: str) -> str:
        """
        å¯¹URLä¸­çš„ä¸­æ–‡å­—ç¬¦è¿›è¡Œç¼–ç 
        
        å‚æ•°:
            url: åŸå§‹URL
        
        è¿”å›:
            ç¼–ç åçš„URL
        """
        def encode_match(match):
            return "".join(f"%{ord(c):02X}" for c in match.group())
            
        return self.CHINESE_PATTERN.sub(encode_match, url)

    def resolve_url(self, base_url: str, relative: str) -> str:
        """
        è§£æç›¸å¯¹URLä¸ºç»å¯¹URL
        
        å‚æ•°:
            base_url: åŸºç¡€URL
            relative: ç›¸å¯¹è·¯å¾„
            
        è¿”å›:
            ç»å¯¹URL
        """
        if relative.startswith(('#', '?')):
            base = base_url.split('#')[0].split('?')[0]
            return base + relative
        
        # æ ‡å‡†åŒ–è·¯å¾„å¤„ç†
        base_parts = list(urlparse(base_url))
        new_path = unquote(relative)
        
        if new_path.startswith('/'):
            base_parts[2] = new_path
        else:
            base_parts[2] = posixpath.normpath(
                posixpath.join(posixpath.dirname(base_parts[2]), new_path)
            )
        
        return urlunparse(base_parts)

    def is_valid_link(self, item: Dict) -> bool:
        """
        éªŒè¯é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
        
        å‚æ•°:
            item: åŒ…å«é“¾æ¥ä¿¡æ¯çš„å­—å…¸
        
        è¿”å›:
            é“¾æ¥æ˜¯å¦æœ‰æ•ˆçš„å¸ƒå°”å€¼
        """
        if not item.get("title") or not item.get("link"):
            return False
            
        link = item["link"]
        
        # æ’é™¤æ— æ•ˆåè®®
        invalid_protocols = ["#", "javascript:", "mailto:", "tel:"]
        if any(link.startswith(proto) for proto in invalid_protocols):
            return False
            
        # è§£æURLè·¯å¾„
        parsed = urlparse(link)
        path = parsed.path
        
        # æ’é™¤æ ¹è·¯å¾„å’Œç´¢å¼•æ–‡ä»¶
        if not path or path == "/":
            return False
            
        filename = path.split("/")[-1].lower()
        if filename in ["index.html", "main.html", "default.html"]:
            return False
            
        # æ’é™¤æ–‡ä»¶ç±»å‹
        file_extensions = ["pdf", "doc", "docx", "xls", "xlsx", "jpg", "jpeg", "png", "gif", "mp3", "mp4"]
        extension = filename.split(".")[-1] if "." in filename else ""
        if extension in file_extensions:
            return False
            
        # æ’é™¤å¯¼èˆªæ–‡æœ¬
        navigation_texts = ["é¦–é¡µ", "ä¸»é¡µ", "ä¸Šé¡µ", "ä¸‹é¡µ", "ä¸‹ä¸€é¡µ", "ä¸Šä¸€é¡µ", "å°¾é¡µ"]
        if item["title"] in navigation_texts:
            return False
            
        return True

    def is_valid_http_link(self, link: str, main_domain: str) -> bool:
        """
        éªŒè¯HTTPé“¾æ¥æ˜¯å¦æœ‰æ•ˆä¸”å±äºä¸»åŸŸå
        
        å‚æ•°:
            link: é“¾æ¥URL
            main_domain: ä¸»åŸŸå
        
        è¿”å›:
            é“¾æ¥æ˜¯å¦æœ‰æ•ˆçš„å¸ƒå°”å€¼
        """
        # æ£€æŸ¥HTTPåè®®
        if not link.startswith(("http://", "https://")):
            logger.info(f"éHTTPé“¾æ¥è¢«è¿‡æ»¤: {link}")
            return False
            
        # è§£æä¸»æœºå
        try:
            host = urlparse(link).hostname
            if not host:
                logger.info(f"æ— æ³•è§£æåŸŸå: {link}")
                return False
                
            # ä¸»åŸŸåéªŒè¯
            if host == main_domain or host.endswith(f".{main_domain}"):
                return True
                
            logger.info(f"éä¸»åŸŸåé“¾æ¥è¢«è¿‡æ»¤: {link} (host: {host}, expected: {main_domain})")
            return False
            
        except Exception:
            return False

    def deduplicate_by_link(self, data: List[Dict]) -> List[Dict]:
        """
        æ ¹æ®é“¾æ¥å»é‡
        
        å‚æ•°:
            data: åŒ…å«é“¾æ¥çš„å­—å…¸åˆ—è¡¨
        
        è¿”å›:
            å»é‡åçš„åˆ—è¡¨
        """
        seen = set()
        result = []
        
        for item in data:
            normalized = self.normalize_url(item["link"])
            if normalized and normalized not in seen:
                seen.add(normalized)
                result.append(item)
                
        return result

    def normalize_url(self, url: str) -> str:
        """
        URLæ ‡å‡†åŒ–
        
        å‚æ•°:
            url: åŸå§‹URL
            
        è¿”å›:
            æ ‡å‡†åŒ–åçš„URL
        """
        if not url:
            return ''
            
        # åŸºæœ¬æ¸…ç†
        url = url.lower().strip()
        url = re.sub(r'#.*$', '', url)  # ç§»é™¤é”šç‚¹
        url = url.rstrip('/')  # ç§»é™¤æœ«å°¾æ–œæ 
        
        # ç»Ÿä¸€åè®®å¤„ç†
        url = url.replace('http://', '').replace('https://', '')
        return url

    # ä»æ–‡ä»¶åŠ è½½HTMLå†…å®¹
    def get_html_from_file(self, url: str, base_path="../runtime/html") -> str:
        host = urlparse(url).netloc
        if not host:
            raise ValueError(f"æ— æ•ˆçš„URL: {url}")

        normalized_url = self.normalize_url(url)
        file_name = hashlib.md5(normalized_url.encode("utf-8")).hexdigest() + ".html"
        file_path = os.path.join(base_path, host, file_name)
        # print("file_path:", file_path)

        if not os.path.exists(file_path):
            return ""
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                return f.read()
        except Exception:
            return ""
        
    def extract_clean_body(self, html_content: str) -> str:
        """
        æå–HTMLä¸­çš„bodyå†…å®¹ï¼Œå¹¶ç§»é™¤å¹²æ‰°æ ‡ç­¾
        
        å‚æ•°:
            html_content: åŸå§‹HTMLå­—ç¬¦ä¸²
            
        è¿”å›:
            æ¸…æ´—åçš„HTMLå­—ç¬¦ä¸²ï¼Œåªä¿ç•™æ­£æ–‡åŒºåŸŸ
        """
        # è§£æHTMLï¼Œå¿½ç•¥é”™è¯¯
        parser = html.HTMLParser(recover=True, encoding='utf-8')
        try:
            tree = html.fromstring(html_content, parser=parser)
        except Exception:
            return ""
        
        # è·å–<body>èŠ‚ç‚¹
        body = tree.xpath('//body')
        if not body:
            return ""
        body = body[0]
        
        # 1. ç§»é™¤<script>ã€<style>ã€<link>ã€<meta>ã€<noscript>æ ‡ç­¾
        remove_tags = ['script', 'style', 'link', 'meta', 'noscript']
        for tag in remove_tags:
            for element in body.xpath(f'.//{tag}'):
                element.getparent().remove(element)
        
        # 2. ç§»é™¤æ³¨é‡Š
        for comment in body.xpath('//comment()'):
            comment.getparent().remove(comment)
        
        # 3. ç§»é™¤å¸¸è§çš„å¤´éƒ¨ã€å°¾éƒ¨ã€å¯¼èˆªç­‰å™ªéŸ³å…ƒç´ 
        noise_selectors = [
            '//*[contains(@class, "footer")]',
            '//*[contains(@class, "header")]',
            '//*[contains(@class, "head")]',
            '//*[contains(@class, "nav")]',
            '//*[contains(@id, "footer")]',
            '//*[contains(@id, "header")]',
            '//*[contains(@id, "nav")]'
        ]
        for selector in noise_selectors:
            for element in body.xpath(selector):
                element.getparent().remove(element)
        
        # 4. æå–bodyçš„innerHTML
        inner_html = ''
        for child in body.getchildren():
            inner_html += html.tostring(child, encoding='unicode', method='html')
        
        # æ¸…é™¤æ ‡ç­¾ä¹‹é—´çš„å¤šä½™æ¢è¡Œå’Œç©ºç™½
        inner_html = re.sub(r'>\s*\r?\n\s*<', '><', inner_html)  # å»é™¤æ ‡ç­¾é—´æ¢è¡Œ
        inner_html = re.sub(r'>\s{2,}<', '><', inner_html)       # æ¸…é™¤è¿ç»­ç©ºæ ¼/åˆ¶è¡¨ç¬¦
        
        return inner_html.strip()        
    
    @staticmethod
    def get_main_domain(url: str) -> str:
        """
        æå–ä¸»åŸŸåï¼Œä¾‹å¦‚ï¼š
        https://news.sina.com.cn/china/xxx â†’ sina.com.cn
        https://sub.blog.example.co.uk â†’ example.co.uk
        """
        extracted = tldextract.extract(url)
        if not extracted.domain or not extracted.suffix:
            return ""  # è§£æå¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return f"{extracted.domain}.{extracted.suffix}"
    
    async def is_url_accessible(url: str, timeout: float = 10.0, retries: int = 2) -> bool:
        """
        åˆ¤æ–­ URL æ˜¯å¦å¯è®¿é—®ï¼Œè‡ªåŠ¨é‡è¯•
        """
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
        }
        for attempt in range(retries + 1):
            try:
                async with httpx.AsyncClient(timeout=timeout, headers=headers, follow_redirects=True) as client:
                    resp = await client.get(url)
                    if 200 <= resp.status_code < 400:
                        return True
            except (httpx.ConnectError, httpx.RequestError, httpx.ReadTimeout) as e:
                print(f"[Attempt {attempt+1}] URLè¿æ¥å¼‚å¸¸: {url} -> {e}")
                await asyncio.sleep(0.5)
        return False

    def generate_urls(self,domain: str) -> list[str]:
        """
        æ ¹æ® domain ç”Ÿæˆå°è¯• URL åˆ—è¡¨ï¼ˆä¼˜å…ˆ httpsï¼Œæ”¯æŒ wwwï¼‰
        """
        domain = domain.strip().rstrip("/")
        urls = [f"https://{domain}", f"http://{domain}"]

        if not domain.lower().startswith("www."):
            urls += [f"https://www.{domain}", f"http://www.{domain}"]

        return urls

    def get_accessible_url(self,domain: str) -> str:
        """
        éå† URL åˆ—è¡¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯è®¿é—®çš„ URL
        """
        urls = self.generate_urls(domain)
        for url in urls:
            if self.is_url_accessible(url):
                return url
        return ''