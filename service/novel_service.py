# service/novel_service.py
import asyncio
from collections import defaultdict
import re
from urllib.parse import urljoin
from parsel import Selector
from service.config_service import ConfigService
from service.crawl_service import CrawlService
import os
import aiofiles
from lxml import html
from lxml.etree import XPathError, ParserError


class NovelService:
    def __init__(self, url: str):
        if url:
            self.url = url
            self.config = ConfigService().load_config(url)
            self.base_url = self.config.get("base_url", "")
        
    # æŠ“å–ç« èŠ‚åˆ—è¡¨é¡µ
    async def fetch_chapter_list(self, url: str):
        async with CrawlService() as crawl:
            result = await crawl.async_fetch_single(url)
            html = result.get("html", "") if result else ""

            if not html:
                return {}

            sel = Selector(html)

            # è·å–ç« èŠ‚åˆ—è¡¨
            chapters_cfg = self.config["chapters"]
            all_chapters = []
            containers = sel.xpath(chapters_cfg["container"])
            
            for container in containers:
                items = container.xpath(chapters_cfg["item"])
                for a in items:
                    title_parts = a.xpath(chapters_cfg["title"]).getall()
                    chap_title = "".join(title_parts).strip()
                    if not chap_title:
                        continue

                    href = a.xpath(chapters_cfg["url"]).get(default="").strip()
                    if not href:
                        continue

                    full_url = urljoin(self.base_url, href)

                    all_chapters.append({
                        "title": chap_title,
                        "url": full_url
                    })
        

            # å»é‡ï¼ˆé˜²æ­¢åˆ†é¡µé‡å¤ï¼‰
            unique_chapters = []
            seen_urls = set()
            for ch in all_chapters:
                if ch["url"] not in seen_urls:
                    unique_chapters.append(ch)
                    seen_urls.add(ch["url"])
            all_chapters = unique_chapters
            
            return all_chapters


    # æŠ“å–å°è¯´ä¿¡æ¯é¡µ
    async def fetch_novel_info(self, url: str):
        async with CrawlService() as crawl:
            result = await crawl.async_fetch_single(url)
            html = result.get("html", "") if result else ""

            if not html:
                return {}

            sel = Selector(html)
            novel_cfg = self.config["novel"]

            title = sel.xpath(novel_cfg["title"]).get(default="").strip()
            author_raw = sel.xpath(novel_cfg["author"]).get(default="")
            author_split = novel_cfg.get("author_split", "ï¼š")
            author = author_raw.split(author_split)[-1].strip() if author_raw else ""

            intro = sel.xpath(novel_cfg["intro"]).get(default="").strip()
            update_raw = sel.xpath(novel_cfg["update_time"]).get(default="")
            update_split = novel_cfg.get("update_split", "ï¼š")
            update_time = update_raw.split(update_split)[-1].strip()

            return {
                "title": title,
                "author": author,
                "intro": intro,
                "update_time": update_time,
            }


    # æŠ“å–å•ç« æ­£æ–‡é¡µï¼ˆå«åˆ†é¡µï¼‰
    async def fetch_chapter_content(self, url: str):
        async with CrawlService() as crawl:
            content_cfg = self.config["content"]
            filters = self.config.get("filters", {})
            chapter_content = []
            title = ""
            base_chapter_id = url.split("/")[-1].split(".")[0]  # å½“å‰ç« èŠ‚ç¼–ç 

            while url:
                result = await crawl.async_fetch_single(url)
                html = result.get("html", "") if result else ""
                if not html:
                    break

                sel = Selector(html)

                # æå–æ ‡é¢˜ï¼Œåªåšä¸€æ¬¡
                if not title:
                    title_sel = sel.xpath(content_cfg["container"])
                    title = title_sel.xpath(content_cfg["title"]).get(default="").strip()

                # æå–æ­£æ–‡
                content_sel = sel.xpath(content_cfg["container"])
                paragraphs = content_sel.xpath(content_cfg["text"]).getall()
                for p in paragraphs:
                    p = p.strip()
                    if p and not any(f in p for f in filters.get("regex", [])):
                        chapter_content.append(p)

                # è·å–ä¸‹ä¸€é¡µ
                next_page = sel.xpath(content_cfg.get("next_page", "")).get()
                if next_page and base_chapter_id in next_page:
                    url = urljoin(self.base_url, next_page)
                else:
                    url = None  # æœ¬ç« åˆ†é¡µç»“æŸ

                print(f"å½“å‰ç« èŠ‚æŠ“å–: {url}ï¼Œä¸‹ä¸€é¡µ: {next_page}")

            return {
                "title": title,
                "content": "\n".join(chapter_content)
            }






















            
    # å¼‚æ­¥ä¸‹è½½æ•´æœ¬å°è¯´ï¼ˆå¤šç« èŠ‚åˆå¹¶ï¼‰
    # ç›´æ¥è°ƒç”¨ fetch_chapter_content()
    async def download_novel(self, novel_name: str, author: str, chapters: list[dict]):
        
        os.makedirs("./output", exist_ok=True)
        file_path = f"./output/{novel_name}_{author}.txt"

        print(f"ğŸ“˜ å¼€å§‹ä¸‹è½½å°è¯´ã€Š{novel_name}ã€‹ï¼ˆå…± {len(chapters)} ç« ï¼‰...")

        merged_text = [f"ã€Š{novel_name}ã€‹ â€”â€” ä½œè€…ï¼š{author}\n\n"]

        # ä¸²è¡ŒæŠ“å–ï¼ˆä¸å¹¶å‘ï¼Œæ›´å®‰å…¨ï¼‰
        for idx, chap in enumerate(chapters, start=1):
            print(f"â¬ ä¸‹è½½ç« èŠ‚ï¼š{chap['title']} - {chap['url']}")
            try:
                data = await self.fetch_chapter_content(chap["url"])
                title = data.get("title") or chap["title"]
                content = data.get("content", "").replace("\\n", "\n").replace("\r", "").strip()
                merged_text.append(f"\n{title}\n\n{content}\n")
                print(f"âœ… æˆåŠŸä¸‹è½½ç« èŠ‚ï¼š{title}")
            except Exception as e:
                print(f"âŒ æŠ“å–ç« èŠ‚å¤±è´¥: {chap['title']} - {e}")
                merged_text.append(f"\n\nç¬¬{idx}ç«  {chap['title']}\n\nã€æŠ“å–å¤±è´¥ã€‘\n")

        # === å†™å…¥æ–‡ä»¶ ===
        async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
            await f.write("".join(merged_text))

        print(f"âœ… å°è¯´ã€Š{novel_name}ã€‹ä¸‹è½½å®Œæˆï¼š{file_path}")
        return file_path



    def compress_html(self, html_content):
        """
        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å‹ç¼©HTMLï¼Œç§»é™¤é‡å¤çš„é“¾æ¥
        """
        # åŒ¹é…æ‰€æœ‰å®¹å™¨
        container_pattern = r'(<div[^>]*>.*?</div>|<ul[^>]*>.*?</ul>|<ol[^>]*>.*?</ol>|<nav[^>]*>.*?</nav>|<section[^>]*>.*?</section>|<article[^>]*>.*?</article>)'
        
        def remove_duplicate_links(match):
            container_html = match.group(0)
            
            # åŒ¹é…å®¹å™¨å†…çš„æ‰€æœ‰é“¾æ¥
            link_pattern = r'(<a\s+[^>]*href=([\'"])(.*?)\2[^>]*>.*?</a>)'
            links = re.findall(link_pattern, container_html, re.DOTALL)
            
            # è®°å½•æ¯ä¸ªhrefå‡ºç°çš„æ¬¡æ•°å’Œç¬¬ä¸€ä¸ªå‡ºç°çš„ä½ç½®
            href_count = defaultdict(int)
            href_first_occurrence = {}
            
            for i, (full_match, quote, href) in enumerate(links):
                href_count[href] += 1
                if href not in href_first_occurrence:
                    href_first_occurrence[href] = (full_match, i)
            
            # æ„å»ºæ–°çš„å®¹å™¨å†…å®¹
            new_container = container_html
            
            # ä»åå¾€å‰å¤„ç†ï¼Œé¿å…ç´¢å¼•å˜åŒ–é—®é¢˜
            for href, count in sorted(href_count.items(), key=lambda x: -x[1]):
                if count > 1:
                    # æ‰¾åˆ°æ‰€æœ‰é‡å¤é“¾æ¥çš„ä½ç½®
                    all_occurrences = [m.start() for m in re.finditer(re.escape(href_first_occurrence[href][0]), new_container)]
                    
                    # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œç§»é™¤å…¶ä»–
                    for i, pos in enumerate(all_occurrences):
                        if i > 0:  # è·³è¿‡ç¬¬ä¸€ä¸ª
                            # æ‰¾åˆ°é“¾æ¥çš„å®Œæ•´åŒ¹é…
                            link_match = re.search(r'<a\s+[^>]*>.*?</a>', new_container[pos:], re.DOTALL)
                            if link_match:
                                link_text_match = re.search(r'>([^<]*)</a>', link_match.group(0))
                                if link_text_match:
                                    # ä¿ç•™é“¾æ¥æ–‡æœ¬ï¼Œä½†ç§»é™¤é“¾æ¥
                                    link_text = link_text_match.group(1)
                                    new_container = new_container[:pos] + link_text + new_container[pos+len(link_match.group(0)):]
            
            return new_container
        
        # å¯¹æ¯ä¸ªå®¹å™¨åº”ç”¨å»é‡
        compressed_html = re.sub(container_pattern, remove_duplicate_links, html_content, flags=re.DOTALL)
        
        return compressed_html
