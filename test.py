import os
from pydantic import BaseModel, Field
from pydantic_ai import Agent
from models.data_models import ChapterListConfig
from service.agent.generator_chapter_agent import XPathGeneratorChapterAgent
from service.agent.generator_content_agent import XPathGeneratorContentAgent
from service.crawl_service import CrawlService

# # è®¾ç½® DeepSeek API å¯†é’¥
# os.environ['DEEPSEEK_API_KEY'] = 'sk-e1e6d80ddacf4e279462a15d96b1d7ba'

# # åˆ›å»ºXPathåˆ†æAgent
# xpath_agent = Agent(
#     'deepseek:deepseek-chat',
#     deps_type=int,
#     output_type=ChapterListConfig,
#     system_prompt=(
#         'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„XPathåˆ†æä¸“å®¶ã€‚'
#         'è¯·ä»”ç»†åˆ†æç”¨æˆ·æä¾›çš„HTMLä»£ç ç»“æ„ï¼Œä»ä¸­è¯†åˆ«å‡ºç« èŠ‚ç›®å½•éƒ¨åˆ†ã€‚'
#         'ä½ çš„ä»»åŠ¡æ˜¯ç”Ÿæˆç²¾ç¡®çš„XPathè¡¨è¾¾å¼æ¥å®šä½ä»¥ä¸‹å…ƒç´ ï¼š'
#         '1. container: åŒ…å«æ‰€æœ‰ç« èŠ‚çš„å®¹å™¨å…ƒç´ '
#         '2. item: å•ä¸ªç« èŠ‚æ¡ç›®'
#         '3. title: ç« èŠ‚æ ‡é¢˜æ–‡æœ¬'
#         '4. url: ç« èŠ‚é“¾æ¥åœ°å€'
#         '5. pagination: åˆ¤æ–­æ˜¯å¦æœ‰åˆ†é¡µï¼ˆtrue/falseï¼‰'
#         '5. more_url: å¦‚æœæœ‰åˆ†é¡µï¼Œæä¾›â€œæ›´å¤šç« èŠ‚â€æˆ–ä¸‹ä¸€é¡µçš„XPathï¼Œå¦åˆ™ä¸ºnullã€‚'
#         'è¯·åŸºäºHTMLçš„å®é™…ç»“æ„ç”ŸæˆXPathï¼Œç¡®ä¿è¡¨è¾¾å¼èƒ½å¤Ÿå‡†ç¡®åŒ¹é…ç›®æ ‡å…ƒç´ ã€‚'
#         'å¯¹äºpaginationå­—æ®µï¼Œå¦‚æœå‘ç°"ä¸‹ä¸€é¡µ"ã€"ä¸‹ä¸€ç« "ç­‰åˆ†é¡µå…ƒç´ ï¼Œè¿”å›trueï¼Œå¦åˆ™è¿”å›falseã€‚'
#     ),
# )


# def main():
#     """ä¸»å‡½æ•°ï¼šè¯»å–HTMLæ–‡ä»¶ï¼Œåˆ†æç»“æ„å¹¶ç”ŸæˆXPathæ¨¡æ¿"""
#     path = './doc/chapter.html'
    
#     try:
#         # è¯»å–æœ¬åœ° HTML æ–‡ä»¶å†…å®¹
#         with open(path, 'r', encoding='utf-8') as file:
#             html_content = file.read()
        
#         print(f"âœ… æˆåŠŸè¯»å–HTMLæ–‡ä»¶ï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
        
#         # åˆ›å»º CrawlService å®ä¾‹å¹¶æ¸…ç†HTML
#         crawl_service = CrawlService()
#         clean_html = crawl_service.extract_clean_body(html_content)
#         print(f"âœ… HTMLæ¸…ç†å®Œæˆï¼Œé•¿åº¦: {len(clean_html)} å­—ç¬¦")
        
#         # æ„å»ºåˆ†ææç¤º
#         prompt = f"""
# è¯·åˆ†æä»¥ä¸‹HTMLä»£ç ç»“æ„ï¼Œç”Ÿæˆç« èŠ‚ç›®å½•çš„XPathé€‰æ‹©å™¨ï¼š

# HTMLå†…å®¹:
# {clean_html}...

# [å®Œæ•´HTMLå†…å®¹å…±{len(clean_html)}å­—ç¬¦]

# è¯·é‡ç‚¹åˆ†æï¼š
# 1. ç« èŠ‚åˆ—è¡¨çš„å®¹å™¨ï¼ˆé€šå¸¸æ˜¯ulã€olã€divç­‰åŒ…å«å¤šä¸ªç« èŠ‚çš„å…ƒç´ ï¼‰
# 2. å•ä¸ªç« èŠ‚æ¡ç›®çš„ç»“æ„
# 3. ç« èŠ‚æ ‡é¢˜çš„æ˜¾ç¤ºæ–¹å¼
# 4. ç« èŠ‚é“¾æ¥çš„å®šä½
# 5. æ˜¯å¦å­˜åœ¨åˆ†é¡µå¯¼èˆª

# è¯·åŸºäºå®é™…HTMLç»“æ„ç”Ÿæˆç²¾ç¡®çš„XPathè¡¨è¾¾å¼ã€‚
# """
        
#         print("ğŸ” å¼€å§‹åˆ†æHTMLç»“æ„å¹¶ç”ŸæˆXPathæ¨¡æ¿...")
        
#         # ä½¿ç”¨Agentåˆ†æHTMLå¹¶ç”ŸæˆXPathæ¨¡æ¿
#         result = xpath_agent.run_sync(prompt)
#         print(result)
#         extracted_template = result.output
        
#         # # è¾“å‡ºç”Ÿæˆçš„æ¨¡æ¿
#         # print("\n" + "="*60)
#         # print("# ç« èŠ‚ç›®å½•XPathæ¨¡æ¿ï¼ˆç« èŠ‚åˆ—è¡¨é¡µï¼‰")
#         # print("="*60)
#         # print("chapters:")
#         # print(f"  container: \"{extracted_template.container}\"")
#         # print(f"  item: \"{extracted_template.item}\"") 
#         # print(f"  title: \"{extracted_template.title}\"")
#         # print(f"  url: \"{extracted_template.url}\"")
#         # print(f"  pagination: {str(extracted_template.pagination).lower()}      # ç« èŠ‚åˆ—è¡¨æ˜¯å¦åˆ†é¡µ")
#         # print(f"  more_url: {str(extracted_template.more_url).lower()}      # ç« èŠ‚åˆ—è¡¨æ˜¯å¦åˆ†é¡µ")
        
#         return extracted_template
        
#     except FileNotFoundError:
#         print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {path}")
#         print("è¯·ç¡®ä¿doc/content.htmlæ–‡ä»¶å­˜åœ¨")
#         return None
#     except Exception as e:
#         print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
#         return None

if __name__ == "__main__":
    
    # è¯»å–æœ¬åœ° HTML æ–‡ä»¶å†…å®¹
    path = './doc/content.html'
    with open(path, 'r', encoding='utf-8') as file:
        html_content = file.read()
    

    print(f"âœ… æˆåŠŸè¯»å–HTMLæ–‡ä»¶ï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
    
    # åˆ›å»º CrawlService å®ä¾‹å¹¶æ¸…ç†HTML
    crawl_service = CrawlService()
    clean_html = crawl_service.extract_clean_body(html_content)
    
    
    model_name='deepseek:deepseek-chat'
    model_key='sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
    generator = XPathGeneratorContentAgent(model_name, model_key)
        
    print(generator.generate_rules(html=clean_html))
    
    
    # template = main()
    
    # if template:
    #     print("\nğŸ‰ XPathæ¨¡æ¿ç”Ÿæˆå®Œæˆï¼")
    #     print("æ‚¨å¯ä»¥å¤åˆ¶ä¸Šé¢çš„æ¨¡æ¿ç”¨äºæ‚¨çš„çˆ¬è™«é…ç½®ã€‚")
    #     print(template)
    # else:
    #     print("\nğŸ’¥ XPathæ¨¡æ¿ç”Ÿæˆå¤±è´¥ã€‚")